{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b939092f-b355-4f0e-88da-51abfb18f585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[K     |████████████████████████████████| 246 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#kafka 라이브러리 설치\n",
    "!pip3 install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a924752-a456-43b1-aa68-4b590160b675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 토픽이 읽을 파일 생성\n",
    "with open(\"new-topic.txt\",\"w\")as f:\n",
    "    for i in range(1,1001):\n",
    "        f.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789f8198-b8a1-4238-976f-5df1e52d8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로듀서 생성(토픽으로 데이터 전송)\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd4961a-1a5e-4b84-b380-563a50fd515e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KafkaProducer in module kafka.producer.kafka:\n",
      "\n",
      "class KafkaProducer(builtins.object)\n",
      " |  KafkaProducer(**configs)\n",
      " |  \n",
      " |  A Kafka client that publishes records to the Kafka cluster.\n",
      " |  \n",
      " |  The producer is thread safe and sharing a single producer instance across\n",
      " |  threads will generally be faster than having multiple instances.\n",
      " |  \n",
      " |  The producer consists of a pool of buffer space that holds records that\n",
      " |  haven't yet been transmitted to the server as well as a background I/O\n",
      " |  thread that is responsible for turning these records into requests and\n",
      " |  transmitting them to the cluster.\n",
      " |  \n",
      " |  :meth:`~kafka.KafkaProducer.send` is asynchronous. When called it adds the\n",
      " |  record to a buffer of pending record sends and immediately returns. This\n",
      " |  allows the producer to batch together individual records for efficiency.\n",
      " |  \n",
      " |  The 'acks' config controls the criteria under which requests are considered\n",
      " |  complete. The \"all\" setting will result in blocking on the full commit of\n",
      " |  the record, the slowest but most durable setting.\n",
      " |  \n",
      " |  If the request fails, the producer can automatically retry, unless\n",
      " |  'retries' is configured to 0. Enabling retries also opens up the\n",
      " |  possibility of duplicates (see the documentation on message\n",
      " |  delivery semantics for details:\n",
      " |  https://kafka.apache.org/documentation.html#semantics\n",
      " |  ).\n",
      " |  \n",
      " |  The producer maintains buffers of unsent records for each partition. These\n",
      " |  buffers are of a size specified by the 'batch_size' config. Making this\n",
      " |  larger can result in more batching, but requires more memory (since we will\n",
      " |  generally have one of these buffers for each active partition).\n",
      " |  \n",
      " |  By default a buffer is available to send immediately even if there is\n",
      " |  additional unused space in the buffer. However if you want to reduce the\n",
      " |  number of requests you can set 'linger_ms' to something greater than 0.\n",
      " |  This will instruct the producer to wait up to that number of milliseconds\n",
      " |  before sending a request in hope that more records will arrive to fill up\n",
      " |  the same batch. This is analogous to Nagle's algorithm in TCP. Note that\n",
      " |  records that arrive close together in time will generally batch together\n",
      " |  even with linger_ms=0 so under heavy load batching will occur regardless of\n",
      " |  the linger configuration; however setting this to something larger than 0\n",
      " |  can lead to fewer, more efficient requests when not under maximal load at\n",
      " |  the cost of a small amount of latency.\n",
      " |  \n",
      " |  The buffer_memory controls the total amount of memory available to the\n",
      " |  producer for buffering. If records are sent faster than they can be\n",
      " |  transmitted to the server then this buffer space will be exhausted. When\n",
      " |  the buffer space is exhausted additional send calls will block.\n",
      " |  \n",
      " |  The key_serializer and value_serializer instruct how to turn the key and\n",
      " |  value objects the user provides into bytes.\n",
      " |  \n",
      " |  Keyword Arguments:\n",
      " |      bootstrap_servers: 'host[:port]' string (or list of 'host[:port]'\n",
      " |          strings) that the producer should contact to bootstrap initial\n",
      " |          cluster metadata. This does not have to be the full node list.\n",
      " |          It just needs to have at least one broker that will respond to a\n",
      " |          Metadata API Request. Default port is 9092. If no servers are\n",
      " |          specified, will default to localhost:9092.\n",
      " |      client_id (str): a name for this client. This string is passed in\n",
      " |          each request to servers and can be used to identify specific\n",
      " |          server-side log entries that correspond to this client.\n",
      " |          Default: 'kafka-python-producer-#' (appended with a unique number\n",
      " |          per instance)\n",
      " |      key_serializer (callable): used to convert user-supplied keys to bytes\n",
      " |          If not None, called as f(key), should return bytes. Default: None.\n",
      " |      value_serializer (callable): used to convert user-supplied message\n",
      " |          values to bytes. If not None, called as f(value), should return\n",
      " |          bytes. Default: None.\n",
      " |      acks (0, 1, 'all'): The number of acknowledgments the producer requires\n",
      " |          the leader to have received before considering a request complete.\n",
      " |          This controls the durability of records that are sent. The\n",
      " |          following settings are common:\n",
      " |  \n",
      " |          0: Producer will not wait for any acknowledgment from the server.\n",
      " |              The message will immediately be added to the socket\n",
      " |              buffer and considered sent. No guarantee can be made that the\n",
      " |              server has received the record in this case, and the retries\n",
      " |              configuration will not take effect (as the client won't\n",
      " |              generally know of any failures). The offset given back for each\n",
      " |              record will always be set to -1.\n",
      " |          1: Wait for leader to write the record to its local log only.\n",
      " |              Broker will respond without awaiting full acknowledgement from\n",
      " |              all followers. In this case should the leader fail immediately\n",
      " |              after acknowledging the record but before the followers have\n",
      " |              replicated it then the record will be lost.\n",
      " |          all: Wait for the full set of in-sync replicas to write the record.\n",
      " |              This guarantees that the record will not be lost as long as at\n",
      " |              least one in-sync replica remains alive. This is the strongest\n",
      " |              available guarantee.\n",
      " |          If unset, defaults to acks=1.\n",
      " |      compression_type (str): The compression type for all data generated by\n",
      " |          the producer. Valid values are 'gzip', 'snappy', 'lz4', 'zstd' or None.\n",
      " |          Compression is of full batches of data, so the efficacy of batching\n",
      " |          will also impact the compression ratio (more batching means better\n",
      " |          compression). Default: None.\n",
      " |      retries (int): Setting a value greater than zero will cause the client\n",
      " |          to resend any record whose send fails with a potentially transient\n",
      " |          error. Note that this retry is no different than if the client\n",
      " |          resent the record upon receiving the error. Allowing retries\n",
      " |          without setting max_in_flight_requests_per_connection to 1 will\n",
      " |          potentially change the ordering of records because if two batches\n",
      " |          are sent to a single partition, and the first fails and is retried\n",
      " |          but the second succeeds, then the records in the second batch may\n",
      " |          appear first.\n",
      " |          Default: 0.\n",
      " |      batch_size (int): Requests sent to brokers will contain multiple\n",
      " |          batches, one for each partition with data available to be sent.\n",
      " |          A small batch size will make batching less common and may reduce\n",
      " |          throughput (a batch size of zero will disable batching entirely).\n",
      " |          Default: 16384\n",
      " |      linger_ms (int): The producer groups together any records that arrive\n",
      " |          in between request transmissions into a single batched request.\n",
      " |          Normally this occurs only under load when records arrive faster\n",
      " |          than they can be sent out. However in some circumstances the client\n",
      " |          may want to reduce the number of requests even under moderate load.\n",
      " |          This setting accomplishes this by adding a small amount of\n",
      " |          artificial delay; that is, rather than immediately sending out a\n",
      " |          record the producer will wait for up to the given delay to allow\n",
      " |          other records to be sent so that the sends can be batched together.\n",
      " |          This can be thought of as analogous to Nagle's algorithm in TCP.\n",
      " |          This setting gives the upper bound on the delay for batching: once\n",
      " |          we get batch_size worth of records for a partition it will be sent\n",
      " |          immediately regardless of this setting, however if we have fewer\n",
      " |          than this many bytes accumulated for this partition we will\n",
      " |          'linger' for the specified time waiting for more records to show\n",
      " |          up. This setting defaults to 0 (i.e. no delay). Setting linger_ms=5\n",
      " |          would have the effect of reducing the number of requests sent but\n",
      " |          would add up to 5ms of latency to records sent in the absence of\n",
      " |          load. Default: 0.\n",
      " |      partitioner (callable): Callable used to determine which partition\n",
      " |          each message is assigned to. Called (after key serialization):\n",
      " |          partitioner(key_bytes, all_partitions, available_partitions).\n",
      " |          The default partitioner implementation hashes each non-None key\n",
      " |          using the same murmur2 algorithm as the java client so that\n",
      " |          messages with the same key are assigned to the same partition.\n",
      " |          When a key is None, the message is delivered to a random partition\n",
      " |          (filtered to partitions with available leaders only, if possible).\n",
      " |      buffer_memory (int): The total bytes of memory the producer should use\n",
      " |          to buffer records waiting to be sent to the server. If records are\n",
      " |          sent faster than they can be delivered to the server the producer\n",
      " |          will block up to max_block_ms, raising an exception on timeout.\n",
      " |          In the current implementation, this setting is an approximation.\n",
      " |          Default: 33554432 (32MB)\n",
      " |      connections_max_idle_ms: Close idle connections after the number of\n",
      " |          milliseconds specified by this config. The broker closes idle\n",
      " |          connections after connections.max.idle.ms, so this avoids hitting\n",
      " |          unexpected socket disconnected errors on the client.\n",
      " |          Default: 540000\n",
      " |      max_block_ms (int): Number of milliseconds to block during\n",
      " |          :meth:`~kafka.KafkaProducer.send` and\n",
      " |          :meth:`~kafka.KafkaProducer.partitions_for`. These methods can be\n",
      " |          blocked either because the buffer is full or metadata unavailable.\n",
      " |          Blocking in the user-supplied serializers or partitioner will not be\n",
      " |          counted against this timeout. Default: 60000.\n",
      " |      max_request_size (int): The maximum size of a request. This is also\n",
      " |          effectively a cap on the maximum record size. Note that the server\n",
      " |          has its own cap on record size which may be different from this.\n",
      " |          This setting will limit the number of record batches the producer\n",
      " |          will send in a single request to avoid sending huge requests.\n",
      " |          Default: 1048576.\n",
      " |      metadata_max_age_ms (int): The period of time in milliseconds after\n",
      " |          which we force a refresh of metadata even if we haven't seen any\n",
      " |          partition leadership changes to proactively discover any new\n",
      " |          brokers or partitions. Default: 300000\n",
      " |      retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
      " |          errors. Default: 100.\n",
      " |      request_timeout_ms (int): Client request timeout in milliseconds.\n",
      " |          Default: 30000.\n",
      " |      receive_buffer_bytes (int): The size of the TCP receive buffer\n",
      " |          (SO_RCVBUF) to use when reading data. Default: None (relies on\n",
      " |          system defaults). Java client defaults to 32768.\n",
      " |      send_buffer_bytes (int): The size of the TCP send buffer\n",
      " |          (SO_SNDBUF) to use when sending data. Default: None (relies on\n",
      " |          system defaults). Java client defaults to 131072.\n",
      " |      socket_options (list): List of tuple-arguments to socket.setsockopt\n",
      " |          to apply to broker connection sockets. Default:\n",
      " |          [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n",
      " |      reconnect_backoff_ms (int): The amount of time in milliseconds to\n",
      " |          wait before attempting to reconnect to a given host.\n",
      " |          Default: 50.\n",
      " |      reconnect_backoff_max_ms (int): The maximum amount of time in\n",
      " |          milliseconds to backoff/wait when reconnecting to a broker that has\n",
      " |          repeatedly failed to connect. If provided, the backoff per host\n",
      " |          will increase exponentially for each consecutive connection\n",
      " |          failure, up to this maximum. Once the maximum is reached,\n",
      " |          reconnection attempts will continue periodically with this fixed\n",
      " |          rate. To avoid connection storms, a randomization factor of 0.2\n",
      " |          will be applied to the backoff resulting in a random range between\n",
      " |          20% below and 20% above the computed value. Default: 1000.\n",
      " |      max_in_flight_requests_per_connection (int): Requests are pipelined\n",
      " |          to kafka brokers up to this number of maximum requests per\n",
      " |          broker connection. Note that if this setting is set to be greater\n",
      " |          than 1 and there are failed sends, there is a risk of message\n",
      " |          re-ordering due to retries (i.e., if retries are enabled).\n",
      " |          Default: 5.\n",
      " |      security_protocol (str): Protocol used to communicate with brokers.\n",
      " |          Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.\n",
      " |          Default: PLAINTEXT.\n",
      " |      ssl_context (ssl.SSLContext): pre-configured SSLContext for wrapping\n",
      " |          socket connections. If provided, all other ssl_* configurations\n",
      " |          will be ignored. Default: None.\n",
      " |      ssl_check_hostname (bool): flag to configure whether ssl handshake\n",
      " |          should verify that the certificate matches the brokers hostname.\n",
      " |          default: true.\n",
      " |      ssl_cafile (str): optional filename of ca file to use in certificate\n",
      " |          veriication. default: none.\n",
      " |      ssl_certfile (str): optional filename of file in pem format containing\n",
      " |          the client certificate, as well as any ca certificates needed to\n",
      " |          establish the certificate's authenticity. default: none.\n",
      " |      ssl_keyfile (str): optional filename containing the client private key.\n",
      " |          default: none.\n",
      " |      ssl_password (str): optional password to be used when loading the\n",
      " |          certificate chain. default: none.\n",
      " |      ssl_crlfile (str): optional filename containing the CRL to check for\n",
      " |          certificate expiration. By default, no CRL check is done. When\n",
      " |          providing a file, only the leaf certificate will be checked against\n",
      " |          this CRL. The CRL can only be checked with Python 3.4+ or 2.7.9+.\n",
      " |          default: none.\n",
      " |      ssl_ciphers (str): optionally set the available ciphers for ssl\n",
      " |          connections. It should be a string in the OpenSSL cipher list\n",
      " |          format. If no cipher can be selected (because compile-time options\n",
      " |          or other configuration forbids use of all the specified ciphers),\n",
      " |          an ssl.SSLError will be raised. See ssl.SSLContext.set_ciphers\n",
      " |      api_version (tuple): Specify which Kafka API version to use. If set to\n",
      " |          None, the client will attempt to infer the broker version by probing\n",
      " |          various APIs. Example: (0, 10, 2). Default: None\n",
      " |      api_version_auto_timeout_ms (int): number of milliseconds to throw a\n",
      " |          timeout exception from the constructor when checking the broker\n",
      " |          api version. Only applies if api_version set to None.\n",
      " |      metric_reporters (list): A list of classes to use as metrics reporters.\n",
      " |          Implementing the AbstractMetricsReporter interface allows plugging\n",
      " |          in classes that will be notified of new metric creation. Default: []\n",
      " |      metrics_num_samples (int): The number of samples maintained to compute\n",
      " |          metrics. Default: 2\n",
      " |      metrics_sample_window_ms (int): The maximum age in milliseconds of\n",
      " |          samples used to compute metrics. Default: 30000\n",
      " |      selector (selectors.BaseSelector): Provide a specific selector\n",
      " |          implementation to use for I/O multiplexing.\n",
      " |          Default: selectors.DefaultSelector\n",
      " |      sasl_mechanism (str): Authentication mechanism when security_protocol\n",
      " |          is configured for SASL_PLAINTEXT or SASL_SSL. Valid values are:\n",
      " |          PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256, SCRAM-SHA-512.\n",
      " |      sasl_plain_username (str): username for sasl PLAIN and SCRAM authentication.\n",
      " |          Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms.\n",
      " |      sasl_plain_password (str): password for sasl PLAIN and SCRAM authentication.\n",
      " |          Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms.\n",
      " |      sasl_kerberos_service_name (str): Service name to include in GSSAPI\n",
      " |          sasl mechanism handshake. Default: 'kafka'\n",
      " |      sasl_kerberos_domain_name (str): kerberos domain name to use in GSSAPI\n",
      " |          sasl mechanism handshake. Default: one of bootstrap servers\n",
      " |      sasl_oauth_token_provider (AbstractTokenProvider): OAuthBearer token provider\n",
      " |          instance. (See kafka.oauth.abstract). Default: None\n",
      " |  \n",
      " |  Note:\n",
      " |      Configuration parameters are described in more detail at\n",
      " |      https://kafka.apache.org/0100/configuration.html#producerconfigs\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __init__(self, **configs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bootstrap_connected(self)\n",
      " |      Return True if the bootstrap is connected.\n",
      " |  \n",
      " |  close(self, timeout=None)\n",
      " |      Close this producer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          timeout (float, optional): timeout in seconds to wait for completion.\n",
      " |  \n",
      " |  flush(self, timeout=None)\n",
      " |      Invoking this method makes all buffered records immediately available\n",
      " |      to send (even if linger_ms is greater than 0) and blocks on the\n",
      " |      completion of the requests associated with these records. The\n",
      " |      post-condition of :meth:`~kafka.KafkaProducer.flush` is that any\n",
      " |      previously sent record will have completed\n",
      " |      (e.g. Future.is_done() == True). A request is considered completed when\n",
      " |      either it is successfully acknowledged according to the 'acks'\n",
      " |      configuration for the producer, or it results in an error.\n",
      " |      \n",
      " |      Other threads can continue sending messages while one thread is blocked\n",
      " |      waiting for a flush call to complete; however, no guarantee is made\n",
      " |      about the completion of messages sent after the flush call begins.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          timeout (float, optional): timeout in seconds to wait for completion.\n",
      " |      \n",
      " |      Raises:\n",
      " |          KafkaTimeoutError: failure to flush buffered records within the\n",
      " |              provided timeout\n",
      " |  \n",
      " |  metrics(self, raw=False)\n",
      " |      Get metrics on producer performance.\n",
      " |      \n",
      " |      This is ported from the Java Producer, for details see:\n",
      " |      https://kafka.apache.org/documentation/#producer_monitoring\n",
      " |      \n",
      " |      Warning:\n",
      " |          This is an unstable interface. It may change in future\n",
      " |          releases without warning.\n",
      " |  \n",
      " |  partitions_for(self, topic)\n",
      " |      Returns set of all known partitions for the topic.\n",
      " |  \n",
      " |  send(self, topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None)\n",
      " |      Publish a message to a topic.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic (str): topic where the message will be published\n",
      " |          value (optional): message value. Must be type bytes, or be\n",
      " |              serializable to bytes via configured value_serializer. If value\n",
      " |              is None, key is required and message acts as a 'delete'.\n",
      " |              See kafka compaction documentation for more details:\n",
      " |              https://kafka.apache.org/documentation.html#compaction\n",
      " |              (compaction requires kafka >= 0.8.1)\n",
      " |          partition (int, optional): optionally specify a partition. If not\n",
      " |              set, the partition will be selected using the configured\n",
      " |              'partitioner'.\n",
      " |          key (optional): a key to associate with the message. Can be used to\n",
      " |              determine which partition to send the message to. If partition\n",
      " |              is None (and producer's partitioner config is left as default),\n",
      " |              then messages with the same key will be delivered to the same\n",
      " |              partition (but if key is None, partition is chosen randomly).\n",
      " |              Must be type bytes, or be serializable to bytes via configured\n",
      " |              key_serializer.\n",
      " |          headers (optional): a list of header key value pairs. List items\n",
      " |              are tuples of str key and bytes value.\n",
      " |          timestamp_ms (int, optional): epoch milliseconds (from Jan 1 1970 UTC)\n",
      " |              to use as the message timestamp. Defaults to current time.\n",
      " |      \n",
      " |      Returns:\n",
      " |          FutureRecordMetadata: resolves to RecordMetadata\n",
      " |      \n",
      " |      Raises:\n",
      " |          KafkaTimeoutError: if unable to fetch topic metadata, or unable\n",
      " |              to obtain memory buffer prior to configured max_block_ms\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DEFAULT_CONFIG = {'acks': 1, 'api_version': None, 'api_version_auto_ti...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KafkaProducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a50ebdb-87a8-47cf-8cef-9534fb03e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageProducer:\n",
    "    broker,topic,producer = \"\",\"\",None\n",
    "    def __init__(self, broker, topic):\n",
    "        self.broker = broker\n",
    "        self.topic = topic\n",
    "        params = {\n",
    "            \"bootstrap_servers\":self.broker,\n",
    "            \"value_serializer\": lambda x : json.dumps(x).encode(\"utf-8\"),\n",
    "            \"acks\":0, #파티션 리더에게 메시지 전달하고 전달이 되었다는 연락을 받지않고 다음 데이터를 전달하러감(높은 전송 속도는 보장하지만 위험도 높 -> 중요하지 않은 경우 사용)\n",
    "            \"api_version\":(2,5,0), #튜플이니 콤마로 표시\n",
    "            \"retries\":3 #재전송 횟수\n",
    "        }\n",
    "        self.producer = KafkaProducer(**params)\n",
    "    def Send_message(self, msg):\n",
    "        try:\n",
    "            future = self.producer.send(self.topic, msg)\n",
    "            self.producer.flush() #메세지; 버퍼를 비워줌\n",
    "            future.get(timeout=60) # 60초 대기\n",
    "            return{\"status_code\":200, \"error\":None}\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "            return e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2c43b24-7125-4805-ad85-a99854d8247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = 'localhost:9092'\n",
    "topic = 'TOPIC1'\n",
    "message_producer = MessageProducer(broker, topic)\n",
    "with open(\"new-topic.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for data in f:\n",
    "        res = message_producer.Send_message(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd21ee7-a122-4287-a207-1de702f1bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
