{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de353bf3-15a4-4a77-b15a-f67cbb913338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 줄 1: pip: 명령어를 찾을 수 없음\n",
      "/bin/bash: 줄 1: pip: 명령어를 찾을 수 없음\n",
      "/bin/bash: 줄 1: pip: 명령어를 찾을 수 없음\n"
     ]
    }
   ],
   "source": [
    "# library install\n",
    "# 필요한 라이브러리 설치\n",
    "!pip install kafka-python\n",
    "!pip install pymysql\n",
    "!pip install cryptography\n",
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "import json\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0a870b-5b71-40db-9dd1-4c496f888031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbConnection():\n",
    "    def __init__(self,\n",
    "                 host='localhost',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        password='password',\n",
    "        db='hadoopguide',\n",
    "        charset='utf8'\n",
    "                ):\n",
    "        self.host = host; self.port=port; self.user=user; self.password=password\n",
    "        self.db=db;self.charset=charset\n",
    "    \n",
    "    # private    \n",
    "    def __connect(self):\n",
    "        self.conn = pymysql.connect(host=self.host,port=self.port,user=self.user,\n",
    "                                    password=self.password,db=self.db,charset=self.charset)                \n",
    "    \n",
    "    def select(self,select_query):\n",
    "        self.__connect()\n",
    "        with self.conn as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                try:\n",
    "                    cursor.execute(select_query)\n",
    "                    return cursor.fetchall()\n",
    "                except Exception as e:\n",
    "                    e.with_traceback()\n",
    "    \n",
    "    def update(self,update_query):\n",
    "        ''' update or insert or delete '''\n",
    "        self.__connect()\n",
    "        with self.conn as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                try:\n",
    "                    result = cursor.execute(update_query)\n",
    "                    conn.commit()\n",
    "                    print(\"success update\" if result!=0 else \"faile update\")\n",
    "                except Exception as e:                    \n",
    "                    e.with_traceback()\n",
    "    def close(self):\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ce44c4-e89c-46dc-948e-5beef8f82da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql - table read\n",
    "dbcon = DbConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed1a7bb8-0166-44a1-a9c5-4455f64ca9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('lee', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('hong', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('kang', 'kang5@gmail.com', '010-0002-0000'),\n",
       " ('lee1', 'lee@gmail.com', '010-0000-0000'),\n",
       " ('hong2', 'hong@gmail.com', '010-0001-0000'),\n",
       " ('kang3', 'kang@gmail.com', '010-0002-0000'))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbcon.select(\"select * from tbl_kafka_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323ff227-f3fb-4466-af7e-9a3197558808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('name', 'varchar(10)', 'YES', '', None, ''),\n",
       " ('email', 'varchar(50)', 'YES', '', None, ''),\n",
       " ('phone', 'varchar(50)', 'YES', '', None, ''))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbcon.select(\"desc tbl_kafka_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35de594c-9ce3-489e-bab0-5ff8fb276d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faile update\n"
     ]
    }
   ],
   "source": [
    "dbcon.update(\"update tbl_kafka_data set email='kang5@gmail.com' where name = 'kang'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f873afdc-6ac8-4a4d-9ee7-05385fdc5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success update\n"
     ]
    }
   ],
   "source": [
    "dbcon.update(\"insert into tbl_kafka_new values ('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee1','lee@gmail.com','010-0000-0000',1),('hong2','hong@gmail.com','010-0001-0000',1),('kang3','kang@gmail.com','010-0002-0000',1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0eecb8fa-fe86-490e-a224-0f52a6fce4de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KafkaConsumer in module kafka.consumer.group:\n",
      "\n",
      "class KafkaConsumer(builtins.object)\n",
      " |  KafkaConsumer(*topics, **configs)\n",
      " |  \n",
      " |  Consume records from a Kafka cluster.\n",
      " |  \n",
      " |  The consumer will transparently handle the failure of servers in the Kafka\n",
      " |  cluster, and adapt as topic-partitions are created or migrate between\n",
      " |  brokers. It also interacts with the assigned kafka Group Coordinator node\n",
      " |  to allow multiple consumers to load balance consumption of topics (requires\n",
      " |  kafka >= 0.9.0.0).\n",
      " |  \n",
      " |  The consumer is not thread safe and should not be shared across threads.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      *topics (str): optional list of topics to subscribe to. If not set,\n",
      " |          call :meth:`~kafka.KafkaConsumer.subscribe` or\n",
      " |          :meth:`~kafka.KafkaConsumer.assign` before consuming records.\n",
      " |  \n",
      " |  Keyword Arguments:\n",
      " |      bootstrap_servers: 'host[:port]' string (or list of 'host[:port]'\n",
      " |          strings) that the consumer should contact to bootstrap initial\n",
      " |          cluster metadata. This does not have to be the full node list.\n",
      " |          It just needs to have at least one broker that will respond to a\n",
      " |          Metadata API Request. Default port is 9092. If no servers are\n",
      " |          specified, will default to localhost:9092.\n",
      " |      client_id (str): A name for this client. This string is passed in\n",
      " |          each request to servers and can be used to identify specific\n",
      " |          server-side log entries that correspond to this client. Also\n",
      " |          submitted to GroupCoordinator for logging with respect to\n",
      " |          consumer group administration. Default: 'kafka-python-{version}'\n",
      " |      group_id (str or None): The name of the consumer group to join for dynamic\n",
      " |          partition assignment (if enabled), and to use for fetching and\n",
      " |          committing offsets. If None, auto-partition assignment (via\n",
      " |          group coordinator) and offset commits are disabled.\n",
      " |          Default: None\n",
      " |      key_deserializer (callable): Any callable that takes a\n",
      " |          raw message key and returns a deserialized key.\n",
      " |      value_deserializer (callable): Any callable that takes a\n",
      " |          raw message value and returns a deserialized value.\n",
      " |      fetch_min_bytes (int): Minimum amount of data the server should\n",
      " |          return for a fetch request, otherwise wait up to\n",
      " |          fetch_max_wait_ms for more data to accumulate. Default: 1.\n",
      " |      fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
      " |          the server will block before answering the fetch request if\n",
      " |          there isn't sufficient data to immediately satisfy the\n",
      " |          requirement given by fetch_min_bytes. Default: 500.\n",
      " |      fetch_max_bytes (int): The maximum amount of data the server should\n",
      " |          return for a fetch request. This is not an absolute maximum, if the\n",
      " |          first message in the first non-empty partition of the fetch is\n",
      " |          larger than this value, the message will still be returned to\n",
      " |          ensure that the consumer can make progress. NOTE: consumer performs\n",
      " |          fetches to multiple brokers in parallel so memory usage will depend\n",
      " |          on the number of brokers containing partitions for the topic.\n",
      " |          Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 MB).\n",
      " |      max_partition_fetch_bytes (int): The maximum amount of data\n",
      " |          per-partition the server will return. The maximum total memory\n",
      " |          used for a request = #partitions * max_partition_fetch_bytes.\n",
      " |          This size must be at least as large as the maximum message size\n",
      " |          the server allows or else it is possible for the producer to\n",
      " |          send messages larger than the consumer can fetch. If that\n",
      " |          happens, the consumer can get stuck trying to fetch a large\n",
      " |          message on a certain partition. Default: 1048576.\n",
      " |      request_timeout_ms (int): Client request timeout in milliseconds.\n",
      " |          Default: 305000.\n",
      " |      retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
      " |          errors. Default: 100.\n",
      " |      reconnect_backoff_ms (int): The amount of time in milliseconds to\n",
      " |          wait before attempting to reconnect to a given host.\n",
      " |          Default: 50.\n",
      " |      reconnect_backoff_max_ms (int): The maximum amount of time in\n",
      " |          milliseconds to backoff/wait when reconnecting to a broker that has\n",
      " |          repeatedly failed to connect. If provided, the backoff per host\n",
      " |          will increase exponentially for each consecutive connection\n",
      " |          failure, up to this maximum. Once the maximum is reached,\n",
      " |          reconnection attempts will continue periodically with this fixed\n",
      " |          rate. To avoid connection storms, a randomization factor of 0.2\n",
      " |          will be applied to the backoff resulting in a random range between\n",
      " |          20% below and 20% above the computed value. Default: 1000.\n",
      " |      max_in_flight_requests_per_connection (int): Requests are pipelined\n",
      " |          to kafka brokers up to this number of maximum requests per\n",
      " |          broker connection. Default: 5.\n",
      " |      auto_offset_reset (str): A policy for resetting offsets on\n",
      " |          OffsetOutOfRange errors: 'earliest' will move to the oldest\n",
      " |          available message, 'latest' will move to the most recent. Any\n",
      " |          other value will raise the exception. Default: 'latest'.\n",
      " |      enable_auto_commit (bool): If True , the consumer's offset will be\n",
      " |          periodically committed in the background. Default: True.\n",
      " |      auto_commit_interval_ms (int): Number of milliseconds between automatic\n",
      " |          offset commits, if enable_auto_commit is True. Default: 5000.\n",
      " |      default_offset_commit_callback (callable): Called as\n",
      " |          callback(offsets, response) response will be either an Exception\n",
      " |          or an OffsetCommitResponse struct. This callback can be used to\n",
      " |          trigger custom actions when a commit request completes.\n",
      " |      check_crcs (bool): Automatically check the CRC32 of the records\n",
      " |          consumed. This ensures no on-the-wire or on-disk corruption to\n",
      " |          the messages occurred. This check adds some overhead, so it may\n",
      " |          be disabled in cases seeking extreme performance. Default: True\n",
      " |      metadata_max_age_ms (int): The period of time in milliseconds after\n",
      " |          which we force a refresh of metadata, even if we haven't seen any\n",
      " |          partition leadership changes to proactively discover any new\n",
      " |          brokers or partitions. Default: 300000\n",
      " |      partition_assignment_strategy (list): List of objects to use to\n",
      " |          distribute partition ownership amongst consumer instances when\n",
      " |          group management is used.\n",
      " |          Default: [RangePartitionAssignor, RoundRobinPartitionAssignor]\n",
      " |      max_poll_records (int): The maximum number of records returned in a\n",
      " |          single call to :meth:`~kafka.KafkaConsumer.poll`. Default: 500\n",
      " |      max_poll_interval_ms (int): The maximum delay between invocations of\n",
      " |          :meth:`~kafka.KafkaConsumer.poll` when using consumer group\n",
      " |          management. This places an upper bound on the amount of time that\n",
      " |          the consumer can be idle before fetching more records. If\n",
      " |          :meth:`~kafka.KafkaConsumer.poll` is not called before expiration\n",
      " |          of this timeout, then the consumer is considered failed and the\n",
      " |          group will rebalance in order to reassign the partitions to another\n",
      " |          member. Default 300000\n",
      " |      session_timeout_ms (int): The timeout used to detect failures when\n",
      " |          using Kafka's group management facilities. The consumer sends\n",
      " |          periodic heartbeats to indicate its liveness to the broker. If\n",
      " |          no heartbeats are received by the broker before the expiration of\n",
      " |          this session timeout, then the broker will remove this consumer\n",
      " |          from the group and initiate a rebalance. Note that the value must\n",
      " |          be in the allowable range as configured in the broker configuration\n",
      " |          by group.min.session.timeout.ms and group.max.session.timeout.ms.\n",
      " |          Default: 10000\n",
      " |      heartbeat_interval_ms (int): The expected time in milliseconds\n",
      " |          between heartbeats to the consumer coordinator when using\n",
      " |          Kafka's group management facilities. Heartbeats are used to ensure\n",
      " |          that the consumer's session stays active and to facilitate\n",
      " |          rebalancing when new consumers join or leave the group. The\n",
      " |          value must be set lower than session_timeout_ms, but typically\n",
      " |          should be set no higher than 1/3 of that value. It can be\n",
      " |          adjusted even lower to control the expected time for normal\n",
      " |          rebalances. Default: 3000\n",
      " |      receive_buffer_bytes (int): The size of the TCP receive buffer\n",
      " |          (SO_RCVBUF) to use when reading data. Default: None (relies on\n",
      " |          system defaults). The java client defaults to 32768.\n",
      " |      send_buffer_bytes (int): The size of the TCP send buffer\n",
      " |          (SO_SNDBUF) to use when sending data. Default: None (relies on\n",
      " |          system defaults). The java client defaults to 131072.\n",
      " |      socket_options (list): List of tuple-arguments to socket.setsockopt\n",
      " |          to apply to broker connection sockets. Default:\n",
      " |          [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n",
      " |      consumer_timeout_ms (int): number of milliseconds to block during\n",
      " |          message iteration before raising StopIteration (i.e., ending the\n",
      " |          iterator). Default block forever [float('inf')].\n",
      " |      security_protocol (str): Protocol used to communicate with brokers.\n",
      " |          Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.\n",
      " |          Default: PLAINTEXT.\n",
      " |      ssl_context (ssl.SSLContext): Pre-configured SSLContext for wrapping\n",
      " |          socket connections. If provided, all other ssl_* configurations\n",
      " |          will be ignored. Default: None.\n",
      " |      ssl_check_hostname (bool): Flag to configure whether ssl handshake\n",
      " |          should verify that the certificate matches the brokers hostname.\n",
      " |          Default: True.\n",
      " |      ssl_cafile (str): Optional filename of ca file to use in certificate\n",
      " |          verification. Default: None.\n",
      " |      ssl_certfile (str): Optional filename of file in pem format containing\n",
      " |          the client certificate, as well as any ca certificates needed to\n",
      " |          establish the certificate's authenticity. Default: None.\n",
      " |      ssl_keyfile (str): Optional filename containing the client private key.\n",
      " |          Default: None.\n",
      " |      ssl_password (str): Optional password to be used when loading the\n",
      " |          certificate chain. Default: None.\n",
      " |      ssl_crlfile (str): Optional filename containing the CRL to check for\n",
      " |          certificate expiration. By default, no CRL check is done. When\n",
      " |          providing a file, only the leaf certificate will be checked against\n",
      " |          this CRL. The CRL can only be checked with Python 3.4+ or 2.7.9+.\n",
      " |          Default: None.\n",
      " |      ssl_ciphers (str): optionally set the available ciphers for ssl\n",
      " |          connections. It should be a string in the OpenSSL cipher list\n",
      " |          format. If no cipher can be selected (because compile-time options\n",
      " |          or other configuration forbids use of all the specified ciphers),\n",
      " |          an ssl.SSLError will be raised. See ssl.SSLContext.set_ciphers\n",
      " |      api_version (tuple): Specify which Kafka API version to use. If set to\n",
      " |          None, the client will attempt to infer the broker version by probing\n",
      " |          various APIs. Different versions enable different functionality.\n",
      " |  \n",
      " |          Examples:\n",
      " |              (0, 9) enables full group coordination features with automatic\n",
      " |                  partition assignment and rebalancing,\n",
      " |              (0, 8, 2) enables kafka-storage offset commits with manual\n",
      " |                  partition assignment only,\n",
      " |              (0, 8, 1) enables zookeeper-storage offset commits with manual\n",
      " |                  partition assignment only,\n",
      " |              (0, 8, 0) enables basic functionality but requires manual\n",
      " |                  partition assignment and offset management.\n",
      " |  \n",
      " |          Default: None\n",
      " |      api_version_auto_timeout_ms (int): number of milliseconds to throw a\n",
      " |          timeout exception from the constructor when checking the broker\n",
      " |          api version. Only applies if api_version set to None.\n",
      " |      connections_max_idle_ms: Close idle connections after the number of\n",
      " |          milliseconds specified by this config. The broker closes idle\n",
      " |          connections after connections.max.idle.ms, so this avoids hitting\n",
      " |          unexpected socket disconnected errors on the client.\n",
      " |          Default: 540000\n",
      " |      metric_reporters (list): A list of classes to use as metrics reporters.\n",
      " |          Implementing the AbstractMetricsReporter interface allows plugging\n",
      " |          in classes that will be notified of new metric creation. Default: []\n",
      " |      metrics_num_samples (int): The number of samples maintained to compute\n",
      " |          metrics. Default: 2\n",
      " |      metrics_sample_window_ms (int): The maximum age in milliseconds of\n",
      " |          samples used to compute metrics. Default: 30000\n",
      " |      selector (selectors.BaseSelector): Provide a specific selector\n",
      " |          implementation to use for I/O multiplexing.\n",
      " |          Default: selectors.DefaultSelector\n",
      " |      exclude_internal_topics (bool): Whether records from internal topics\n",
      " |          (such as offsets) should be exposed to the consumer. If set to True\n",
      " |          the only way to receive records from an internal topic is\n",
      " |          subscribing to it. Requires 0.10+ Default: True\n",
      " |      sasl_mechanism (str): Authentication mechanism when security_protocol\n",
      " |          is configured for SASL_PLAINTEXT or SASL_SSL. Valid values are:\n",
      " |          PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256, SCRAM-SHA-512.\n",
      " |      sasl_plain_username (str): username for sasl PLAIN and SCRAM authentication.\n",
      " |          Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms.\n",
      " |      sasl_plain_password (str): password for sasl PLAIN and SCRAM authentication.\n",
      " |          Required if sasl_mechanism is PLAIN or one of the SCRAM mechanisms.\n",
      " |      sasl_kerberos_service_name (str): Service name to include in GSSAPI\n",
      " |          sasl mechanism handshake. Default: 'kafka'\n",
      " |      sasl_kerberos_domain_name (str): kerberos domain name to use in GSSAPI\n",
      " |          sasl mechanism handshake. Default: one of bootstrap servers\n",
      " |      sasl_oauth_token_provider (AbstractTokenProvider): OAuthBearer token provider\n",
      " |          instance. (See kafka.oauth.abstract). Default: None\n",
      " |  \n",
      " |  Note:\n",
      " |      Configuration parameters are described in more detail at\n",
      " |      https://kafka.apache.org/documentation/#consumerconfigs\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *topics, **configs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __next__(self)\n",
      " |  \n",
      " |  assign(self, partitions)\n",
      " |      Manually assign a list of TopicPartitions to this consumer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partitions (list of TopicPartition): Assignment for this instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          IllegalStateError: If consumer has already called\n",
      " |          :meth:`~kafka.KafkaConsumer.subscribe`.\n",
      " |      \n",
      " |      Warning:\n",
      " |          It is not possible to use both manual partition assignment with\n",
      " |          :meth:`~kafka.KafkaConsumer.assign` and group assignment with\n",
      " |          :meth:`~kafka.KafkaConsumer.subscribe`.\n",
      " |      \n",
      " |      Note:\n",
      " |          This interface does not support incremental assignment and will\n",
      " |          replace the previous assignment (if there was one).\n",
      " |      \n",
      " |      Note:\n",
      " |          Manual topic assignment through this method does not use the\n",
      " |          consumer's group management functionality. As such, there will be\n",
      " |          no rebalance operation triggered when group membership or cluster\n",
      " |          and topic metadata change.\n",
      " |  \n",
      " |  assignment(self)\n",
      " |      Get the TopicPartitions currently assigned to this consumer.\n",
      " |      \n",
      " |      If partitions were directly assigned using\n",
      " |      :meth:`~kafka.KafkaConsumer.assign`, then this will simply return the\n",
      " |      same partitions that were previously assigned.  If topics were\n",
      " |      subscribed using :meth:`~kafka.KafkaConsumer.subscribe`, then this will\n",
      " |      give the set of topic partitions currently assigned to the consumer\n",
      " |      (which may be None if the assignment hasn't happened yet, or if the\n",
      " |      partitions are in the process of being reassigned).\n",
      " |      \n",
      " |      Returns:\n",
      " |          set: {TopicPartition, ...}\n",
      " |  \n",
      " |  beginning_offsets(self, partitions)\n",
      " |      Get the first offset for the given partitions.\n",
      " |      \n",
      " |      This method does not change the current consumer position of the\n",
      " |      partitions.\n",
      " |      \n",
      " |      Note:\n",
      " |          This method may block indefinitely if the partition does not exist.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partitions (list): List of TopicPartition instances to fetch\n",
      " |              offsets for.\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``{TopicPartition: int}``: The earliest available offsets for the\n",
      " |          given partitions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          UnsupportedVersionError: If the broker does not support looking\n",
      " |              up the offsets by timestamp.\n",
      " |          KafkaTimeoutError: If fetch failed in request_timeout_ms.\n",
      " |  \n",
      " |  bootstrap_connected(self)\n",
      " |      Return True if the bootstrap is connected.\n",
      " |  \n",
      " |  close(self, autocommit=True)\n",
      " |      Close the consumer, waiting indefinitely for any needed cleanup.\n",
      " |      \n",
      " |      Keyword Arguments:\n",
      " |          autocommit (bool): If auto-commit is configured for this consumer,\n",
      " |              this optional flag causes the consumer to attempt to commit any\n",
      " |              pending consumed offsets prior to close. Default: True\n",
      " |  \n",
      " |  commit(self, offsets=None)\n",
      " |      Commit offsets to kafka, blocking until success or error.\n",
      " |      \n",
      " |      This commits offsets only to Kafka. The offsets committed using this API\n",
      " |      will be used on the first fetch after every rebalance and also on\n",
      " |      startup. As such, if you need to store offsets in anything other than\n",
      " |      Kafka, this API should not be used. To avoid re-processing the last\n",
      " |      message read if a consumer is restarted, the committed offset should be\n",
      " |      the next message your application should consume, i.e.: last_offset + 1.\n",
      " |      \n",
      " |      Blocks until either the commit succeeds or an unrecoverable error is\n",
      " |      encountered (in which case it is thrown to the caller).\n",
      " |      \n",
      " |      Currently only supports kafka-topic offset storage (not zookeeper).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          offsets (dict, optional): {TopicPartition: OffsetAndMetadata} dict\n",
      " |              to commit with the configured group_id. Defaults to currently\n",
      " |              consumed offsets for all subscribed partitions.\n",
      " |  \n",
      " |  commit_async(self, offsets=None, callback=None)\n",
      " |      Commit offsets to kafka asynchronously, optionally firing callback.\n",
      " |      \n",
      " |      This commits offsets only to Kafka. The offsets committed using this API\n",
      " |      will be used on the first fetch after every rebalance and also on\n",
      " |      startup. As such, if you need to store offsets in anything other than\n",
      " |      Kafka, this API should not be used. To avoid re-processing the last\n",
      " |      message read if a consumer is restarted, the committed offset should be\n",
      " |      the next message your application should consume, i.e.: last_offset + 1.\n",
      " |      \n",
      " |      This is an asynchronous call and will not block. Any errors encountered\n",
      " |      are either passed to the callback (if provided) or discarded.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          offsets (dict, optional): {TopicPartition: OffsetAndMetadata} dict\n",
      " |              to commit with the configured group_id. Defaults to currently\n",
      " |              consumed offsets for all subscribed partitions.\n",
      " |          callback (callable, optional): Called as callback(offsets, response)\n",
      " |              with response as either an Exception or an OffsetCommitResponse\n",
      " |              struct. This callback can be used to trigger custom actions when\n",
      " |              a commit request completes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          kafka.future.Future\n",
      " |  \n",
      " |  committed(self, partition, metadata=False)\n",
      " |      Get the last committed offset for the given partition.\n",
      " |      \n",
      " |      This offset will be used as the position for the consumer\n",
      " |      in the event of a failure.\n",
      " |      \n",
      " |      This call may block to do a remote call if the partition in question\n",
      " |      isn't assigned to this consumer or if the consumer hasn't yet\n",
      " |      initialized its cache of committed offsets.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partition (TopicPartition): The partition to check.\n",
      " |          metadata (bool, optional): If True, return OffsetAndMetadata struct\n",
      " |              instead of offset int. Default: False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The last committed offset (int or OffsetAndMetadata), or None if there was no prior commit.\n",
      " |  \n",
      " |  end_offsets(self, partitions)\n",
      " |      Get the last offset for the given partitions. The last offset of a\n",
      " |      partition is the offset of the upcoming message, i.e. the offset of the\n",
      " |      last available message + 1.\n",
      " |      \n",
      " |      This method does not change the current consumer position of the\n",
      " |      partitions.\n",
      " |      \n",
      " |      Note:\n",
      " |          This method may block indefinitely if the partition does not exist.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partitions (list): List of TopicPartition instances to fetch\n",
      " |              offsets for.\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``{TopicPartition: int}``: The end offsets for the given partitions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          UnsupportedVersionError: If the broker does not support looking\n",
      " |              up the offsets by timestamp.\n",
      " |          KafkaTimeoutError: If fetch failed in request_timeout_ms\n",
      " |  \n",
      " |  highwater(self, partition)\n",
      " |      Last known highwater offset for a partition.\n",
      " |      \n",
      " |      A highwater offset is the offset that will be assigned to the next\n",
      " |      message that is produced. It may be useful for calculating lag, by\n",
      " |      comparing with the reported position. Note that both position and\n",
      " |      highwater refer to the *next* offset -- i.e., highwater offset is\n",
      " |      one greater than the newest available message.\n",
      " |      \n",
      " |      Highwater offsets are returned in FetchResponse messages, so will\n",
      " |      not be available if no FetchRequests have been sent for this partition\n",
      " |      yet.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partition (TopicPartition): Partition to check\n",
      " |      \n",
      " |      Returns:\n",
      " |          int or None: Offset if available\n",
      " |  \n",
      " |  metrics(self, raw=False)\n",
      " |      Get metrics on consumer performance.\n",
      " |      \n",
      " |      This is ported from the Java Consumer, for details see:\n",
      " |      https://kafka.apache.org/documentation/#consumer_monitoring\n",
      " |      \n",
      " |      Warning:\n",
      " |          This is an unstable interface. It may change in future\n",
      " |          releases without warning.\n",
      " |  \n",
      " |  next_v1(self)\n",
      " |  \n",
      " |  next_v2(self)\n",
      " |  \n",
      " |  offsets_for_times(self, timestamps)\n",
      " |      Look up the offsets for the given partitions by timestamp. The\n",
      " |      returned offset for each partition is the earliest offset whose\n",
      " |      timestamp is greater than or equal to the given timestamp in the\n",
      " |      corresponding partition.\n",
      " |      \n",
      " |      This is a blocking call. The consumer does not have to be assigned the\n",
      " |      partitions.\n",
      " |      \n",
      " |      If the message format version in a partition is before 0.10.0, i.e.\n",
      " |      the messages do not have timestamps, ``None`` will be returned for that\n",
      " |      partition. ``None`` will also be returned for the partition if there\n",
      " |      are no messages in it.\n",
      " |      \n",
      " |      Note:\n",
      " |          This method may block indefinitely if the partition does not exist.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          timestamps (dict): ``{TopicPartition: int}`` mapping from partition\n",
      " |              to the timestamp to look up. Unit should be milliseconds since\n",
      " |              beginning of the epoch (midnight Jan 1, 1970 (UTC))\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``{TopicPartition: OffsetAndTimestamp}``: mapping from partition\n",
      " |          to the timestamp and offset of the first message with timestamp\n",
      " |          greater than or equal to the target timestamp.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the target timestamp is negative\n",
      " |          UnsupportedVersionError: If the broker does not support looking\n",
      " |              up the offsets by timestamp.\n",
      " |          KafkaTimeoutError: If fetch failed in request_timeout_ms\n",
      " |  \n",
      " |  partitions_for_topic(self, topic)\n",
      " |      This method first checks the local metadata cache for information\n",
      " |      about the topic. If the topic is not found (either because the topic\n",
      " |      does not exist, the user is not authorized to view the topic, or the\n",
      " |      metadata cache is not populated), then it will issue a metadata update\n",
      " |      call to the cluster.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic (str): Topic to check.\n",
      " |      \n",
      " |      Returns:\n",
      " |          set: Partition ids\n",
      " |  \n",
      " |  pause(self, *partitions)\n",
      " |      Suspend fetching from the requested partitions.\n",
      " |      \n",
      " |      Future calls to :meth:`~kafka.KafkaConsumer.poll` will not return any\n",
      " |      records from these partitions until they have been resumed using\n",
      " |      :meth:`~kafka.KafkaConsumer.resume`.\n",
      " |      \n",
      " |      Note: This method does not affect partition subscription. In particular,\n",
      " |      it does not cause a group rebalance when automatic assignment is used.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          *partitions (TopicPartition): Partitions to pause.\n",
      " |  \n",
      " |  paused(self)\n",
      " |      Get the partitions that were previously paused using\n",
      " |      :meth:`~kafka.KafkaConsumer.pause`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          set: {partition (TopicPartition), ...}\n",
      " |  \n",
      " |  poll(self, timeout_ms=0, max_records=None, update_offsets=True)\n",
      " |      Fetch data from assigned topics / partitions.\n",
      " |      \n",
      " |      Records are fetched and returned in batches by topic-partition.\n",
      " |      On each poll, consumer will try to use the last consumed offset as the\n",
      " |      starting offset and fetch sequentially. The last consumed offset can be\n",
      " |      manually set through :meth:`~kafka.KafkaConsumer.seek` or automatically\n",
      " |      set as the last committed offset for the subscribed list of partitions.\n",
      " |      \n",
      " |      Incompatible with iterator interface -- use one or the other, not both.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          timeout_ms (int, optional): Milliseconds spent waiting in poll if\n",
      " |              data is not available in the buffer. If 0, returns immediately\n",
      " |              with any records that are available currently in the buffer,\n",
      " |              else returns empty. Must not be negative. Default: 0\n",
      " |          max_records (int, optional): The maximum number of records returned\n",
      " |              in a single call to :meth:`~kafka.KafkaConsumer.poll`.\n",
      " |              Default: Inherit value from max_poll_records.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict: Topic to list of records since the last fetch for the\n",
      " |              subscribed list of topics and partitions.\n",
      " |  \n",
      " |  position(self, partition)\n",
      " |      Get the offset of the next record that will be fetched\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partition (TopicPartition): Partition to check\n",
      " |      \n",
      " |      Returns:\n",
      " |          int: Offset\n",
      " |  \n",
      " |  resume(self, *partitions)\n",
      " |      Resume fetching from the specified (paused) partitions.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          *partitions (TopicPartition): Partitions to resume.\n",
      " |  \n",
      " |  seek(self, partition, offset)\n",
      " |      Manually specify the fetch offset for a TopicPartition.\n",
      " |      \n",
      " |      Overrides the fetch offsets that the consumer will use on the next\n",
      " |      :meth:`~kafka.KafkaConsumer.poll`. If this API is invoked for the same\n",
      " |      partition more than once, the latest offset will be used on the next\n",
      " |      :meth:`~kafka.KafkaConsumer.poll`.\n",
      " |      \n",
      " |      Note: You may lose data if this API is arbitrarily used in the middle of\n",
      " |      consumption to reset the fetch offsets.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          partition (TopicPartition): Partition for seek operation\n",
      " |          offset (int): Message offset in partition\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If offset is not an int >= 0; or if partition is not\n",
      " |              currently assigned.\n",
      " |  \n",
      " |  seek_to_beginning(self, *partitions)\n",
      " |      Seek to the oldest available offset for partitions.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          *partitions: Optionally provide specific TopicPartitions, otherwise\n",
      " |              default to all assigned partitions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If any partition is not currently assigned, or if\n",
      " |              no partitions are assigned.\n",
      " |  \n",
      " |  seek_to_end(self, *partitions)\n",
      " |      Seek to the most recent available offset for partitions.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          *partitions: Optionally provide specific TopicPartitions, otherwise\n",
      " |              default to all assigned partitions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If any partition is not currently assigned, or if\n",
      " |              no partitions are assigned.\n",
      " |  \n",
      " |  subscribe(self, topics=(), pattern=None, listener=None)\n",
      " |      Subscribe to a list of topics, or a topic regex pattern.\n",
      " |      \n",
      " |      Partitions will be dynamically assigned via a group coordinator.\n",
      " |      Topic subscriptions are not incremental: this list will replace the\n",
      " |      current assignment (if there is one).\n",
      " |      \n",
      " |      This method is incompatible with :meth:`~kafka.KafkaConsumer.assign`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics (list): List of topics for subscription.\n",
      " |          pattern (str): Pattern to match available topics. You must provide\n",
      " |              either topics or pattern, but not both.\n",
      " |          listener (ConsumerRebalanceListener): Optionally include listener\n",
      " |              callback, which will be called before and after each rebalance\n",
      " |              operation.\n",
      " |      \n",
      " |              As part of group management, the consumer will keep track of the\n",
      " |              list of consumers that belong to a particular group and will\n",
      " |              trigger a rebalance operation if one of the following events\n",
      " |              trigger:\n",
      " |      \n",
      " |              * Number of partitions change for any of the subscribed topics\n",
      " |              * Topic is created or deleted\n",
      " |              * An existing member of the consumer group dies\n",
      " |              * A new member is added to the consumer group\n",
      " |      \n",
      " |              When any of these events are triggered, the provided listener\n",
      " |              will be invoked first to indicate that the consumer's assignment\n",
      " |              has been revoked, and then again when the new assignment has\n",
      " |              been received. Note that this listener will immediately override\n",
      " |              any listener set in a previous call to subscribe. It is\n",
      " |              guaranteed, however, that the partitions revoked/assigned\n",
      " |              through this interface are from topics subscribed in this call.\n",
      " |      \n",
      " |      Raises:\n",
      " |          IllegalStateError: If called after previously calling\n",
      " |              :meth:`~kafka.KafkaConsumer.assign`.\n",
      " |          AssertionError: If neither topics or pattern is provided.\n",
      " |          TypeError: If listener is not a ConsumerRebalanceListener.\n",
      " |  \n",
      " |  subscription(self)\n",
      " |      Get the current topic subscription.\n",
      " |      \n",
      " |      Returns:\n",
      " |          set: {topic, ...}\n",
      " |  \n",
      " |  topics(self)\n",
      " |      Get all topics the user is authorized to view.\n",
      " |      This will always issue a remote call to the cluster to fetch the latest\n",
      " |      information.\n",
      " |      \n",
      " |      Returns:\n",
      " |          set: topics\n",
      " |  \n",
      " |  unsubscribe(self)\n",
      " |      Unsubscribe from all topics and clear all assigned partitions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DEFAULT_CONFIG = {'api_version': None, 'api_version_auto_timeout_ms': ...\n",
      " |  \n",
      " |  DEFAULT_SESSION_TIMEOUT_MS_0_9 = 30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KafkaConsumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25eca607-1ec2-4c54-aa77-33f22cf1ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaP(KafkaProducer):\n",
    "    def __init__(self,**args):\n",
    "        super().__init__(**args)        \n",
    "    def send_message(self,topic,data):\n",
    "        self.send(topic,value=data)\n",
    "        self.flush()    \n",
    "        \n",
    "class KafkaC(KafkaConsumer):\n",
    "    def __init__(self,topic,**args):\n",
    "        super().__init__(topic,**args)\n",
    "    def receive_message(self,fnProcess):\n",
    "        for msg in self:\n",
    "            fnProcess(msg.value)\n",
    "            self.commit()\n",
    "            \n",
    "            \n",
    "# for class test\n",
    "p_args = {\n",
    "    'bootstrap_servers' : \"localhost:9092\",\n",
    "    'value_serializer' : lambda v: json.dumps(v).encode('utf-8')\n",
    "}\n",
    "c_args = {    \n",
    "    'bootstrap_servers' : 'localhost:9092',\n",
    "    'auto_offset_reset' : 'earliest',\n",
    "    'value_deserializer' : lambda v: json.loads(v.decode('utf-8')),\n",
    "    'consumer_timeout_ms' : 1000*5,\n",
    "    'group_id' : 'g1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d01dbb-ab3d-4757-99a8-41e7ee38eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce\n",
    "pkafa = KafkaP(**p_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75372126-86ca-4c48-a226-cccb9efde9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer\n",
    "topic = 'temp_topic'\n",
    "ckafa = KafkaC(topic,**c_args)\n",
    "ckafa.receive_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e934badc-6d49-47be-92d2-4ba248433cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'email', 'phone']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db->procuder send -> consumer receive\n",
    "# dataframe\n",
    "dbcon = DbConnection()\n",
    "result = dbcon.select(\"select * from tbl_kafka_data\")\n",
    "colnames = dbcon.select(\"desc tbl_kafka_data\")\n",
    "[ row[0] for row in colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6cbab18-3457-4fd1-8992-ba07f55d7bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result,columns=[ row[0] for row in colnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a614da9-bb21-4bef-a0ba-bea5bda91215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing  데이터프레임 데이터 변경\n",
    "# 메일 서비스 업체별 회원수  - new table - data insert  or gmail = 1 and another = 0\n",
    "df['email_type']= df.apply(lambda row : 1 if row['email'].split('@')[-1] == 'gmail.com' else 0   ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "466ed9fc-861b-4058-8f28-df6d90850076",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>email_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lee</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hong</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kang</td>\n",
       "      <td>kang5@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lee1</td>\n",
       "      <td>lee@gmail.com</td>\n",
       "      <td>010-0000-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hong2</td>\n",
       "      <td>hong@gmail.com</td>\n",
       "      <td>010-0001-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kang3</td>\n",
       "      <td>kang@gmail.com</td>\n",
       "      <td>010-0002-0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name            email          phone  email_type\n",
       "0    kang  kang5@gmail.com  010-0002-0000           1\n",
       "1    hong   hong@gmail.com  010-0001-0000           1\n",
       "2    hong   hong@gmail.com  010-0001-0000           1\n",
       "3     lee    lee@gmail.com  010-0000-0000           1\n",
       "4     lee    lee@gmail.com  010-0000-0000           1\n",
       "5    kang  kang5@gmail.com  010-0002-0000           1\n",
       "6     lee    lee@gmail.com  010-0000-0000           1\n",
       "7    kang  kang5@gmail.com  010-0002-0000           1\n",
       "8     lee    lee@gmail.com  010-0000-0000           1\n",
       "9    hong   hong@gmail.com  010-0001-0000           1\n",
       "10   kang  kang5@gmail.com  010-0002-0000           1\n",
       "11   hong   hong@gmail.com  010-0001-0000           1\n",
       "12   kang  kang5@gmail.com  010-0002-0000           1\n",
       "13    lee    lee@gmail.com  010-0000-0000           1\n",
       "14   hong   hong@gmail.com  010-0001-0000           1\n",
       "15   hong   hong@gmail.com  010-0001-0000           1\n",
       "16    lee    lee@gmail.com  010-0000-0000           1\n",
       "17   kang  kang5@gmail.com  010-0002-0000           1\n",
       "18    lee    lee@gmail.com  010-0000-0000           1\n",
       "19   hong   hong@gmail.com  010-0001-0000           1\n",
       "20   kang  kang5@gmail.com  010-0002-0000           1\n",
       "21    lee    lee@gmail.com  010-0000-0000           1\n",
       "22   hong   hong@gmail.com  010-0001-0000           1\n",
       "23   kang  kang5@gmail.com  010-0002-0000           1\n",
       "24   lee1    lee@gmail.com  010-0000-0000           1\n",
       "25  hong2   hong@gmail.com  010-0001-0000           1\n",
       "26  kang3   kang@gmail.com  010-0002-0000           1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89834fa7-25fb-474e-8f8f-fbc6cff60054",
   "metadata": {},
   "source": [
    "# df 데이터 새로운 테이블로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e92007e-35c6-44ca-b433-45a1c8ba134c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['create table IF NOT EXISTS tbl_kafka_new2(name varchar(50), email varchar(50), phone varchar(50), email_type int)',\n",
       " \"insert into tbl_kafka_new2 values('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee1','lee@gmail.com','010-0000-0000',1),('hong2','hong@gmail.com','010-0001-0000',1),('kang3','kang@gmail.com','010-0002-0000',1)\"]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# producer\n",
    "create_table = []\n",
    "create_table.append('create table IF NOT EXISTS tbl_kafka_new2(name varchar(50), email varchar(50), phone varchar(50), email_type int)')\n",
    "strdata = \"insert into tbl_kafka_new2 values\"\n",
    "for index , row in df.iterrows():\n",
    "    temp = \"\"\n",
    "    temp = \"(\" if index == 0 else \",(\"\n",
    "    temp += ','.join(f\"'{item}'\" if not isinstance(item, int) else str(item) for item in row.values )\n",
    "    temp += \")\"\n",
    "    strdata += temp\n",
    "create_table.append(strdata)\n",
    "create_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0a15a6eb-1628-48e5-bdfe-1637c185134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkafa.send_message(topic, create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "437f0619-0dc1-4609-9526-5c36cbcf020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "ckafa.receive_message(lambda x : print(x))\n",
    "# for message in consumer:\n",
    "#     sql = message.value.decode('utf-8')  # 메시지 디코딩\n",
    "#     sql = sql.strip('\"')\n",
    "#     print(f\"Topic: {message.topic}, Partition: {message.partition}, Offset: {message.offset}, Key: {message.key}, Value: {message.value}\")\n",
    "#     dbcon.update(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133efe5-ce1c-47dd-bd9d-389cd51d80e6",
   "metadata": {},
   "source": [
    "## 디비연결해서 업데이트\n",
    "**강사님 코드** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d8c55ae2-4a2e-40f3-bb3f-25f04342d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkafa.send_message(topic, create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ee1aba69-d232-4f09-9fc7-58f653273dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_consumer(x):\n",
    "    dbcon = DbConnection()    \n",
    "    for data in x:        \n",
    "        print(f\"update for query : {data}\")\n",
    "        dbcon.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "41f21238-5657-4781-825d-17a564bec63b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update for query : create table IF NOT EXISTS tbl_kafka_new2(name varchar(50), email varchar(50), phone varchar(50), email_type int)\n",
      "faile update\n",
      "update for query : insert into tbl_kafka_new2 values('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee1','lee@gmail.com','010-0000-0000',1),('hong2','hong@gmail.com','010-0001-0000',1),('kang3','kang@gmail.com','010-0002-0000',1)\n",
      "success update\n"
     ]
    }
   ],
   "source": [
    "ckafa.receive_message(lambda x : process_consumer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c4d59-bd38-448a-9695-b6bf4aaba453",
   "metadata": {},
   "source": [
    "## 내가짠 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ac329e12-7473-45cd-90c6-81665fea581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkafa.send_message(topic, create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0dbff1f0-3f89-44ac-9bbc-4cc79efc10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: temp_topic, Partition: 0, Offset: 35, Key: None, Value: ['create table IF NOT EXISTS tbl_kafka_new2(name varchar(50), email varchar(50), phone varchar(50), email_type int)', \"insert into tbl_kafka_new2 values('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee1','lee@gmail.com','010-0000-0000',1),('hong2','hong@gmail.com','010-0001-0000',1),('kang3','kang@gmail.com','010-0002-0000',1)\"]\n",
      "\n",
      "업데이트 쿼리 create table IF NOT EXISTS tbl_kafka_new2(name varchar(50), email varchar(50), phone varchar(50), email_type int)\n",
      "faile update\n",
      "\n",
      "업데이트 쿼리 insert into tbl_kafka_new2 values('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee','lee@gmail.com','010-0000-0000',1),('hong','hong@gmail.com','010-0001-0000',1),('kang','kang5@gmail.com','010-0002-0000',1),('lee1','lee@gmail.com','010-0000-0000',1),('hong2','hong@gmail.com','010-0001-0000',1),('kang3','kang@gmail.com','010-0002-0000',1)\n",
      "success update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in ckafa:\n",
    "    print(f\"Topic: {message.topic}, Partition: {message.partition}, Offset: {message.offset}, Key: {message.key}, Value: {message.value}\")\n",
    "    print()\n",
    "    for msg in message.value:\n",
    "        try:\n",
    "            print(f'업데이트 쿼리 {msg}')\n",
    "            sql = msg\n",
    "            dbcon.update(sql)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4d8297-2faa-4332-bbae-d29194ed53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer -DB update\n",
    "# insert value형태 ('kang','kang5@gmail.com','010-0002-0000',1),('kang','kang5@gmail.com','010-0002-0000',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a22b2a-78a9-42ea-a320-92565ed0c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index , row in df.iterrows():\n",
    "#     temp = \"\"\n",
    "#     temp = \"(\" if index == 0 else \",(\"\n",
    "#     temp += ','.join(f\"'{item}'\" if not isinstance(item, int) else str(item) for item in row.values )\n",
    "#     temp += \")\"\n",
    "#     strdata += temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc4752-0a14-449f-afde-b2581c10a0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
